services:
  kafka:
    image: apache/kafka:3.7.1
    environment:
      - KAFKA_NODE_ID=1
      - KAFKA_CLUSTER_ID=8aXXFOpmR5GoEZ72mvIszQ
      - KAFKA_PROCESS_ROLES=broker,controller
      - KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_LOG_DIRS=/var/lib/kafka/data
      - KAFKA_METADATA_LOG_DIR=/var/lib/kafka/metadata
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1
    healthcheck:
      test: ["CMD", "kafka-topics", "--list", "--bootstrap-server", "kafka:9092"]
      interval: 30s
      timeout: 10s
      retries: 4
    volumes:
      - ./kafka/data:/var/lib/kafka/data
      - ./kafka/metadata:/var/lib/kafka/metadata
    networks:
      - stock_network

  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - "9003:9000"
    environment:
      KAFKA_BROKERCONNECT: "kafka:9092"
    networks:
      - stock_network

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
    #change
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - stock_network

  producer:
    build:
      context: .
      dockerfile: Dockerfile.producer
    container_name: stock-producer
    depends_on:
      - kafka
    environment:
      FINNHUB_API_KEY: ${FINNHUB_API_KEY}
      KAFKA_BROKER: kafka:9092
    restart: unless-stopped
    networks:
      - stock_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  consumer:
    build:
      context: .
      dockerfile: Dockerfile.consumer
    container_name: stock-consumer
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_BROKER: kafka:9092
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    restart: unless-stopped
    networks:
      - stock_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  airflow-init:
    image: apache/airflow:latest
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTION_API_SERVER_URL=http://airflow-api-server:8080/execution/
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - _PIP_ADDITIONAL_REQUIREMENTS=yfinance pandas apache-airflow-providers-snowflake
      - DOCKER_HOST=tcp://host.docker.internal:2375
      - AIRFLOW__API_AUTH__JWT_SECRET=e56dd19451adc48c5824618773255470ad6ac101524ae29bc840e03466267aa4
      - AIRFLOW__CORE__TEST_CONNECTION=Enabled
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      resources:
        limits:
          memory: 2g
    command: >
      bash -c "airflow db migrate"
    networks:
      - stock_network

  airflow-api-server:
    image: apache/airflow:latest
    container_name: airflow-api-server
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_started
      airflow-init:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          memory: 4g
        reservations:
          memory: 2g
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTION_API_SERVER_URL=http://airflow-api-server:8080/execution/
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS=True
      - _PIP_ADDITIONAL_REQUIREMENTS=yfinance pandas apache-airflow-providers-snowflake
      - DOCKER_HOST=tcp://host.docker.internal:2375
      - AIRFLOW__API_AUTH__JWT_SECRET=e56dd19451adc48c5824618773255470ad6ac101524ae29bc840e03466267aa4
      - AIRFLOW__CORE__FERNET_KEY=Sbs0WgsO2W7yCuH9q7LA6CoRoyUM1bqG_tID6qjaQW8=
      - AIRFLOW__CORE__TEST_CONNECTION=Enabled
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: api-server
    networks:
      - stock_network

  airflow-scheduler:
    image: apache/airflow:latest
    container_name: airflow-scheduler
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_started
      airflow-init:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          memory: 8g     
        reservations:
          memory: 4g
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTION_API_SERVER_URL=http://airflow-api-server:8080/execution/
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - _PIP_ADDITIONAL_REQUIREMENTS=yfinance pandas apache-airflow-providers-snowflake
      - DOCKER_HOST=tcp://host.docker.internal:2375
      - AIRFLOW__API_AUTH__JWT_SECRET=e56dd19451adc48c5824618773255470ad6ac101524ae29bc840e03466267aa4
      - AIRFLOW__CORE__FERNET_KEY=Sbs0WgsO2W7yCuH9q7LA6CoRoyUM1bqG_tID6qjaQW8=
      - AIRFLOW__CORE__TEST_CONNECTION=Enabled
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    networks:
      - stock_network

  airflow-dag-processor:
    image: apache/airflow:latest
    container_name: airflow-dag-processor
    command: dag-processor
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_started
      airflow-init:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          memory: 6g
        reservations:
          memory: 3g
    restart: always
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTION_API_SERVER_URL=http://airflow-api-server:8080/execution/
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - _PIP_ADDITIONAL_REQUIREMENTS=yfinance pandas apache-airflow-providers-snowflake
      - DOCKER_HOST=tcp://host.docker.internal:2375
      - AIRFLOW__API_AUTH__JWT_SECRET=e56dd19451adc48c5824618773255470ad6ac101524ae29bc840e03466267aa4
      - AIRFLOW__CORE__FERNET_KEY=Sbs0WgsO2W7yCuH9q7LA6CoRoyUM1bqG_tID6qjaQW8=
      - AIRFLOW__CORE__TEST_CONNECTION=Enabled
      - DOCKER_HOST=unix:///var/run/docker.sock
    networks:
      - stock_network

  postgres:
    image: postgres:15
    container_name: airflow-postgres
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
    - postgres_data:/var/lib/postgresql/data
    ports:
    - "5432:5432"
    networks:
    - stock_network
    deploy:
      resources:
        limits:
          memory: 2g
        reservations:
          memory: 1g
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  dbt:
    build:
      context: .
      dockerfile: Dockerfile.dbt
    container_name: dbt
    volumes:
      - ./dbt:/dbt
      - ./dbt_logs:/root/.dbt
    environment:
      - DBT_PROFILES_DIR=/dbt
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
    working_dir: /dbt
    networks:
      - stock_network
    restart: unless-stopped
    command: tail -f /dev/null

volumes:
  postgres_data:
  minio_data:
  kafka_data:
  kafka_metadata:

networks:
  stock_network:
    driver: bridge